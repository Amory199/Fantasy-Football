<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>M3 Evaluation Master Guide (Graph-RAG)</title>
  <style>
    :root {
      --bg: #0b1020;
      --paper: #ffffff;
      --ink: #111827;
      --muted: #4b5563;
      --rule: #e5e7eb;
      --accent: #1f6feb;
      --accent2: #0ea5e9;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji", "Segoe UI Emoji";
    }

    @page { margin: 18mm; }

    body {
      margin: 0;
      font-family: var(--sans);
      background: #f3f4f6;
      color: var(--ink);
    }

    .wrap {
      max-width: 980px;
      margin: 32px auto;
      padding: 0 16px;
    }

    .paper {
      background: var(--paper);
      border: 1px solid var(--rule);
      border-radius: 10px;
      padding: 28px 34px;
      box-shadow: 0 10px 30px rgba(0,0,0,0.08);
    }

    h1 {
      font-size: 28px;
      margin: 0 0 6px 0;
      letter-spacing: -0.02em;
    }
    .subtitle {
      color: var(--muted);
      margin: 0 0 14px 0;
      line-height: 1.35;
    }

    h2 {
      margin: 26px 0 10px 0;
      font-size: 18px;
      border-top: 1px solid var(--rule);
      padding-top: 18px;
    }

    h3 {
      margin: 16px 0 8px 0;
      font-size: 15px;
    }

    p, li {
      line-height: 1.55;
      font-size: 13.5px;
    }

    ul { margin: 8px 0 10px 18px; }

    .callout {
      border-left: 4px solid var(--accent);
      background: #eff6ff;
      padding: 10px 12px;
      border-radius: 6px;
      margin: 10px 0;
    }

    .bad {
      border-left-color: #dc2626;
      background: #fef2f2;
    }

    code {
      font-family: var(--mono);
      background: #f3f4f6;
      padding: 1px 5px;
      border-radius: 4px;
      font-size: 12.5px;
    }

    pre {
      background: #0b1020;
      color: #e5e7eb;
      border-radius: 8px;
      padding: 12px;
      overflow: auto;
      font-family: var(--mono);
      font-size: 12px;
      line-height: 1.45;
      margin: 10px 0;
    }

    .grid2 {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 14px;
    }

    .kpi {
      border: 1px solid var(--rule);
      border-radius: 8px;
      padding: 10px 12px;
      background: #fafafa;
    }

    .kpi b { display: block; margin-bottom: 4px; }

    .toc {
      border: 1px solid var(--rule);
      border-radius: 8px;
      padding: 10px 12px;
      background: #fcfcfd;
    }
    .toc a { color: var(--accent); text-decoration: none; }
    .toc a:hover { text-decoration: underline; }

    .footer {
      margin-top: 24px;
      color: var(--muted);
      font-size: 12px;
    }

    @media print {
      body { background: white; }
      .wrap { margin: 0; max-width: none; }
      .paper { box-shadow: none; border: none; border-radius: 0; }
      a { color: black; text-decoration: none; }
    }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="paper">
      <h1>Milestone 3 (ACL) — Ultra‑Detailed Evaluation Master Guide (Graph‑RAG)</h1>
      <p class="subtitle">
        One document to make your team evaluation‑proof: what each component does, how it works in <b>your code</b>,
        what to present, what to demo, what they can ask in Q&amp;A, and how to recover from failures.
      </p>

      <div class="grid2">
        <div class="kpi">
          <b>Evaluation Slot</b>
          <div>45 minutes per team</div>
          <div>Team Presentation: 18–22 min (15%)</div>
          <div>Individual Q&A: remaining time (15%)</div>
        </div>
        <div class="kpi">
          <b>Project</b>
          <div>FantasyTrivia — FPL Graph‑RAG</div>
          <div>Hybrid retrieval: Cypher + embeddings</div>
          <div>UI: Streamlit</div>
        </div>
      </div>

      <div class="toc" style="margin-top:14px;">
        <b>Table of Contents</b>
        <ul>
          <li><a href="#nonneg">0) Non‑Negotiables</a></li>
          <li><a href="#system">1) Your System</a></li>
          <li><a href="#assign">2) Component Assignments</a></li>
          <li><a href="#c1">3) Component 1 — Input Preprocessing</a></li>
          <li><a href="#c2">4) Component 2 — Retrieval Layer</a></li>
          <li><a href="#c3">5) Component 3 — LLM Layer</a></li>
          <li><a href="#c4">6) Component 4 — UI + Integration</a></li>
          <li><a href="#errors">7) Error Analysis & Improvements</a></li>
          <li><a href="#script">8) Presentation Script (18–22 min)</a></li>
          <li><a href="#qa">9) Individual Q&A Prep</a></li>
          <li><a href="#check">10) No‑Room‑for‑Error Checklist</a></li>
          <li><a href="#cmd">11) Commands</a></li>
          <li><a href="#pdf">12) Export to PDF</a></li>
        </ul>
      </div>

      <h2 id="nonneg">0) Non‑Negotiables (Read First)</h2>
      <div class="callout">
        <b>What you must show</b>
        <ul>
          <li>Fully integrated pipeline: input → preprocessing → baseline + embeddings retrieval → context → LLM → UI answer.</li>
          <li>Baseline retrieval: ≥10 Cypher query templates that answer ≥10 question types.</li>
          <li>Embedding retrieval: choose one approach (node OR feature vectors) and compare ≥2 embedding models.</li>
          <li>LLM prompt: structured (context + persona + task) and compare ≥3 models (qual + quant).</li>
          <li>UI: users ask questions, see KG context, see final answer, UI remains functional after answering.</li>
        </ul>
      </div>

      <div class="callout bad">
        <b>What NOT to do</b>
        <ul>
          <li>Don’t explain lab concepts (Neo4j basics, what embeddings are, what an LLM is).</li>
          <li>Don’t add intro/motivation/related work. Only what you implemented.</li>
          <li>Don’t demo isolated components. Must show full integration.</li>
        </ul>
      </div>

      <h2 id="system">1) Your System (Exactly What You Built)</h2>
      <h3>1.1 One‑slide pipeline summary (say in 20 seconds)</h3>
      <p>
        The user asks an FPL question in Streamlit. We preprocess the input (intent, entities, and a query embedding), then retrieve
        information from Neo4j using two strategies: (1) baseline Cypher templates and (2) embedding similarity.
        We combine both into a structured context and pass it to an LLM with persona + instructions. The UI shows the retrieved context
        and the final answer, and stays usable for multiple questions.
      </p>

      <h3>1.2 Code map (always know where things are)</h3>
      <ul>
        <li>UI: <code>app.py</code></li>
        <li>Preprocessing: <code>src/preprocessing.py</code> (<code>InputPreprocessor</code>)</li>
        <li>Retrieval orchestration: <code>src/retrieval.py</code> (<code>GraphRetriever</code>)</li>
        <li>Baseline query library: <code>src/cypher_queries.py</code> (<code>CypherQueryLibrary</code>)</li>
        <li>Embedding creation/storage: <code>src/create_embeddings.py</code> (<code>PlayerEmbedder</code>)</li>
        <li>LLM layer: <code>src/llm_layer.py</code> (<code>LLMLayer</code>)</li>
      </ul>

      <h3>1.3 What the backend returns (integration proof)</h3>
      <div class="callout">
        For each user query, the pipeline produces one combined output containing:
        <ul>
          <li><code>intent</code></li>
          <li><code>cypher_results</code> (structured KG rows)</li>
          <li><code>similar_players</code> (embedding similarity rows)</li>
          <li><code>embedding_type</code> (<code>text</code> or <code>numerical</code>)</li>
          <li><code>query</code> (original question)</li>
        </ul>
        This is what you show in the UI to prove baseline + embeddings are both executed.
      </div>

      <h2 id="assign">2) Component Assignments (4 Owners)</h2>
      <div class="callout">
        Each member owns exactly one component and must explain it line‑by‑line in Q&amp;A. Only the asked person answers.
      </div>
      <ul>
        <li>Component 1 — Input Preprocessing (Owner: ________)</li>
        <li>Component 2 — Graph Retrieval Layer (Owner: ________)</li>
        <li>Component 3 — LLM Layer (Owner: ________)</li>
        <li>Component 4 — UI + Integration (Owner: ________)</li>
      </ul>

      <h2 id="c1">3) Component 1 — Input Preprocessing</h2>
      <h3>3.1 Purpose and outputs</h3>
      <p>
        Preprocessing converts raw natural language into structured signals that retrieval can execute.
        Entry point: <code>InputPreprocessor.preprocess(query)</code>.
      </p>
      <ul>
        <li><b>Intent</b>: a single label (9 supported types).</li>
        <li><b>Entities</b>: players/positions/seasons/stats/numbers.</li>
        <li><b>Query embedding</b>: 384‑dim vector for semantic retrieval.</li>
      </ul>

      <h3>3.2 Intent classification (how it works)</h3>
      <p>
        In <code>classify_intent()</code>, the query is lowercased, each intent gets a score based on keyword hits,
        and the intent with the highest score is returned. If none match, it falls back to <code>general_question</code>.
      </p>
      <div class="callout">
        <b>Defensible rationale (say this)</b>
        <ul>
          <li>Deterministic, low latency, no external API failure modes.</li>
          <li>Easy to debug during evaluation.</li>
          <li>Improved iteratively by adding missing keywords (e.g., least/fewest/lowest/worst).</li>
        </ul>
      </div>

      <h3>3.3 Entity extraction (what you extract and why)</h3>
      <p>
        Implemented in <code>extract_entities()</code>. Player names are detected by grouping consecutive capitalized words.
        Positions are normalized to Neo4j labels (GK/DEF/MID/FWD). Seasons and stat keywords are extracted if present.
      </p>
      <div class="callout">
        <b>Known limitation (say confidently)</b>
        <ul>
          <li>If the user types names fully lowercase (“haaland”), extraction may miss it.</li>
          <li>Mitigation: Cypher uses <code>CONTAINS</code> and semantic text embedding retrieval still returns relevant players.</li>
        </ul>
      </div>

      <h3>3.4 Input embedding</h3>
      <p>
        Uses SentenceTransformer <code>all-MiniLM-L6-v2</code> on <b>CPU</b> to produce a 384‑dim query vector.
        CPU is used for stability and reproducibility.
      </p>

      <h3>3.5 Error analysis & improvements (mandatory slide)</h3>
      <ul>
        <li>Entity extraction missed names at sentence start → fixed by scanning all positions for consecutive capitalized words.</li>
        <li>“least/fewest/lowest/worst” questions misclassified → added keywords to the <code>top_players</code> intent.</li>
      </ul>

      <h2 id="c2">4) Component 2 — Graph Retrieval Layer</h2>
      <h3>4.1 Baseline retrieval (Cypher) — what happens step-by-step</h3>
      <p>
        In <code>GraphRetriever.baseline_retrieval()</code>: we preprocess the query to get intent/entities,
        select the correct Cypher template, then execute it on Neo4j and return structured rows.
      </p>
      <div class="callout">
        <b>Evaluation requirement mapping</b>
        <ul>
          <li>“Use Cypher queries” → all baseline answers come from <code>CypherQueryLibrary</code>.</li>
          <li>“≥10 templates” → you can present at least 10 distinct query functions.</li>
          <li>“Pass extracted entities” → player/position/season values guide query selection and filtering.</li>
        </ul>
      </div>

      <h3>4.2 Cypher query templates (≥10) — what to present</h3>
      <p>
        Implemented in <code>src/cypher_queries.py</code>. Show a slide table: question type → query function → output columns.
        Highlight robustness: player queries use <code>CONTAINS</code> + limits to avoid empty results.
      </p>

      <h3>4.3 Embedding retrieval — chosen approach and two models</h3>
      <div class="callout">
        <b>Chosen embedding approach:</b> feature vector embeddings (player representations)
        <ul>
          <li><b>Model A:</b> Numerical embeddings using 13 normalized season stats.</li>
          <li><b>Model B:</b> Text embeddings using SentenceTransformer over generated player descriptions.</li>
        </ul>
        <b>Explainable difference:</b> numerical similarity = performance profile; text similarity = semantic match to query wording.
      </div>

      <h3>4.4 Hybrid retrieval (baseline + embeddings combined)</h3>
      <p>
        <code>GraphRetriever.hybrid_retrieval()</code> returns a single combined object containing intent, KG rows, similar players,
        and embedding type. This is your strongest integration evidence.
      </p>

      <h2 id="c3">5) Component 3 — LLM Layer</h2>
      <h3>5.1 Context construction (what the model sees)</h3>
      <p>
        The LLM receives a context string created by <code>format_context_for_llm()</code>.
        It includes the original question, the classified intent, structured KG rows, and embedding similarity results.
      </p>

      <h3>5.2 Prompt structure (persona + context + task)</h3>
      <p>
        In <code>LLMLayer.create_prompt()</code>, you enforce grounding by telling the model: “use only the context” and “cite statistics”.
        This matches the evaluation requirement for a structured prompt.
      </p>
      <pre>You are a Fantasy Premier League expert assistant...

Context from Knowledge Graph:
...

User Question: ...

Instructions:
- Answer using only context
- Cite relevant statistics
- If context is insufficient, say so

Answer:</pre>

      <h3>5.3 3‑model comparison (qualitative + quantitative)</h3>
      <p>
        In <code>LLMLayer.MODELS</code> you compare 3 HuggingFace models: FLAN‑T5 Large, Falcon 7B Instruct, and Phi‑2.
      </p>
      <ul>
        <li><b>Quantitative (required):</b> response time (seconds) and success/failure rate.</li>
        <li><b>Qualitative (required):</b> faithfulness to context, specificity, readability, conciseness.</li>
      </ul>
      <div class="callout">
        <b>Important limitation to state</b>
        <ul>
          <li>Serverless inference can rate‑limit or fail without a token.</li>
          <li>Mitigation: token input, retry, and focusing on end-to-end integration evidence.</li>
        </ul>
      </div>

      <h2 id="c4">6) Component 4 — UI + Integration (Streamlit)</h2>
      <p>
        UI is implemented in <code>app.py</code> with three modes: Chat (full pipeline), Compare Embeddings (two embedding models),
        Graph Stats (live Neo4j queries). The sidebar toggles embedding type, shows retrieved context, and selects the LLM model.
      </p>
      <div class="callout">
        <b>Must show in demo (map directly to rubric)</b>
        <ul>
          <li>User types a question (Chat mode input).</li>
          <li>Show KG context (enable “Show Retrieved Context”).</li>
          <li>Show final answer (LLM output panel).</li>
          <li>Ask multiple questions (proves UI stays functional).</li>
        </ul>
      </div>

      <h2 id="errors">7) Error Analysis & Improvements (Mandatory Slide)</h2>
      <p>Use “Issue → root cause → fix → outcome”. Keep each line specific.</p>
      <ul>
        <li><b>Entity extraction failure</b> → names at sentence start missed → fixed by scanning all positions for consecutive capitalized words.</li>
        <li><b>Exact match caused no results</b> → strict matching → changed player lookups to <code>CONTAINS</code> with safe limits.</li>
        <li><b>Missing dependency</b> (plotly) → environment mismatch → installed via requirements / pip.</li>
        <li><b>WinError 10013 port blocked</b> → OS blocked low ports → run Streamlit on high port (e.g., 9090).</li>
      </ul>

      <h2 id="script">8) Presentation Script (18–22 Minutes)</h2>
      <ul>
        <li><b>0:00–2:00</b> Architecture</li>
        <li><b>2:00–4:00</b> Preprocessing</li>
        <li><b>4:00–7:00</b> Baseline retrieval + 10+ queries</li>
        <li><b>7:00–10:00</b> Embeddings (2 models)</li>
        <li><b>10:00–14:00</b> LLM layer + 3 models comparison</li>
        <li><b>14:00–16:00</b> Error analysis</li>
        <li><b>16:00–21:00</b> Live demo (integrated pipeline)</li>
      </ul>

      <h2 id="qa">9) Individual Q&A Prep (per component)</h2>
      <div class="grid2">
        <div class="kpi">
          <b>Component 1</b>
          <ul>
            <li>Explain intent scoring.</li>
            <li>Show entity extraction logic + edge cases.</li>
            <li>Why CPU embeddings.</li>
          </ul>
        </div>
        <div class="kpi">
          <b>Component 2</b>
          <ul>
            <li>Show dispatcher: intent/entities → query.</li>
            <li>Explain 2 embedding models + similarity.</li>
            <li>Explain hybrid retrieval output.</li>
          </ul>
        </div>
      </div>
      <div class="grid2" style="margin-top: 12px;">
        <div class="kpi">
          <b>Component 3</b>
          <ul>
            <li>Prompt structure + grounding rule.</li>
            <li>3-model comparison (time + quality rubric).</li>
            <li>Failure modes + fallback.</li>
          </ul>
        </div>
        <div class="kpi">
          <b>Component 4</b>
          <ul>
            <li>How UI calls retriever + LLM.</li>
            <li>Session state rationale.</li>
            <li>How UI stays functional after answers.</li>
          </ul>
        </div>
      </div>

      <h2 id="check">10) No‑Room‑for‑Error Checklist</h2>
      <div class="callout">
        <b>Before evaluation</b>
        <ul>
          <li>Install deps: <code>python -m pip install -r requirements.txt</code></li>
          <li>Run app on safe port: <code>streamlit run app.py --server.port 9090</code></li>
          <li>Open <code>http://localhost:9090</code></li>
          <li>Ask 2 questions end-to-end.</li>
          <li>Confirm Neo4j credentials in <code>config.txt</code></li>
        </ul>
      </div>

      <h2 id="cmd">11) Commands (Copy/Paste)</h2>
      <pre>.venv\Scripts\Activate.ps1

python -m pip install -r requirements.txt

streamlit run app.py --server.port 9090</pre>

      <h2 id="pdf">12) Export to PDF</h2>
      <p><b>Option A:</b> Open this HTML in a browser → Ctrl+P → Save as PDF.</p>
      <p><b>Option B:</b> Run the PowerShell export script: <code>tools\export_master_guide_pdf.ps1</code></p>

      <div class="footer">
        Ultra‑detailed guide aligned to your codebase (FantasyTrivia Graph‑RAG). Use it as a rehearsal checklist.
      </div>
    </div>
  </div>
</body>
</html>
