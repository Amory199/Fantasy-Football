# Milestone 3 (ACL) â€” Ultraâ€‘Detailed Evaluation Master Guide (Graphâ€‘RAG)
**Goal of this document:** Make your team *evaluationâ€‘proof*. It explains **every part** of the evaluation requirements and **every component** in your implemented system (FantasyTrivia) with:
- what it does,
- how it works in *your code*,
- what to present in slides,
- what to show in the live demo,
- what they can ask in Q&A,
- failure modes + recovery steps.

This is written to match the â€œdonâ€™t explain lab conceptsâ€ rule: we focus only on what you built and how it satisfies the rubric.

**Evaluation slot:** 45 minutes (team)
- **Team Presentation (15%)**: **18â€“22 minutes** (strict)
- **Individual Q&A (15%)**: remaining time; **only the asked member answers**

**Project in this workspace:** *FantasyTrivia* â€” Graphâ€‘RAG for Fantasy Premier League (FPL)

---

## 0) Nonâ€‘Negotiables (Rules + How to Avoid Losing Points)

### 0.1 What they are grading (translate guideline â†’ checklist)
You must demonstrate **all** of the following **live**:

1) **Fully integrated pipeline**
   - Input â†’ preprocessing â†’ retrieval (baseline + embeddings) â†’ context â†’ LLM â†’ answer â†’ UI.

2) **Input preprocessing** includes (must be shown):
   - System overview of preprocessing outputs
   - Intent classification
   - Entity extraction
   - Input embedding (if used in your retrieval)
   - Error analysis + improvement attempts

3) **Graph retrieval layer**
   - **Baseline:** Cypher queries
     - at least **10 query templates** that answer **10 questions**
     - extracted entities must be used to query the KG
   - **Embeddings:** choose **one approach** and compare **â‰¥2 embedding models**
     - You chose **feature vector embeddings** and implemented **two models**:
       - numerical feature vectors
       - text embeddings

4) **LLM layer**
   - combines baseline + embedding results into one context
   - uses structured prompt: **context + persona + task/instructions**
   - compares **â‰¥3 models**
   - comparison includes **quantitative + qualitative** impressions

5) **UI**
   - user can type a question (and/or select a question)
   - user can view KGâ€‘retrieved context
   - user can view final LLM answer
   - full backend integration
   - UI stays functional after answering (multiple questions in a row)

### 0.2 What NOT to do (to avoid autoâ€‘deductions)
- Donâ€™t teach lab material (what is Neo4j, what is RAG). They already know.
- Donâ€™t do intro, motivation, related work.
- Donâ€™t show only a diagram or only text.
- Donâ€™t demo isolated pieces.

### 0.3 Live demo stability rules (how to prevent â€œrandom failureâ€)
Before you walk into evaluation:
- Run the app on a safe port (Windows sometimes blocks 8501):
  - `streamlit run app.py --server.port 9090`
- Confirm the app opens in browser.
- Run at least two questions endâ€‘toâ€‘end.
- Confirm `config.txt` credentials are correct.

---

## 1) System Architecture (Exactly What You Built)

### 1.1 Oneâ€‘slide pipeline (say this in 20 seconds)
â€œThe user asks a natural language FPL question in Streamlit. We preprocess the input (intent, entities, embedding), then retrieve information from the Neo4j knowledge graph using **two retrieval strategies**: (1) baseline Cypher queries and (2) embedding similarity search. We combine both into a structured context and pass it into an LLM prompt with persona + instructions. The UI shows both the retrieved context and the final answer, and remains interactive after each query.â€

### 1.2 Code map (always know where things are)
- UI: `app.py`
- Preprocessing: `src/preprocessing.py` (`InputPreprocessor`)
- Retrieval orchestration: `src/retrieval.py` (`GraphRetriever`)
- Baseline Cypher templates: `src/cypher_queries.py` (`CypherQueryLibrary`)
- Embedding creation + storage: `src/create_embeddings.py` (`PlayerEmbedder`)
- LLM layer: `src/llm_layer.py` (`LLMLayer`)

### 1.3 Ground truth: what your pipeline returns
For each user query, your backend returns a single structured dict:
- `intent`: one label
- `cypher_results`: list of dicts
- `similar_players`: list of dicts (player + similarity)
- `embedding_type`: `text` or `numerical`
- `query`: original user string

This is the â€œintegration evidenceâ€ you show in demo (because it proves baseline + embeddings both executed).

---

## 2) Component Ownership (4 Team Members)
Fill this table and keep it in your slides (one small slide is enough):

| Component | Owner | Must explain in Q&A |
|---|---|---|
| 1) Input Preprocessing | ________ | intent + entity extraction + embedding + errors |
| 2) Graph Retrieval Layer | ________ | baseline Cypher + embeddings + hybrid retrieval |
| 3) LLM Layer | ________ | prompt structure + 3-model comparison |
| 4) UI + Integration | ________ | Streamlit modes + backend calls + stability |

**Rule:** In Q&A, only the asked person answers.

---

## 3) Component 1 â€” Input Preprocessing (Everything You Must Know)

### 3.1 Purpose (what preprocessing must achieve)
Preprocessing must convert **raw language** into **structured signals** usable by retrieval:
- intent: which type of question is it?
- entities: which player/team/position/season/stat is being referenced?
- embedding: a semantic representation of the query text

In your code this is: `InputPreprocessor.preprocess(query)`.

### 3.2 Intent classification (implementation detail)
**File:** `src/preprocessing.py`  
**Method:** `classify_intent(query: str) -> str`

**Algorithm**
1) Lowercase the query.
2) For each intent category, count how many intent keywords appear as substrings.
3) Pick the intent with the maximum score.
4) If none match, return `general_question`.

**Intent labels you support (9)**
- `player_stats`
- `team_query`
- `top_players`
- `fixture_query`
- `position_query`
- `season_query`
- `comparison`
- `recommendation`
- `general_question`

**What to say in evaluation (why this is valid)**
- Deterministic + fast + no external dependency.
- Debuggable under evaluation conditions.
- Handles the required scope of question types for the chosen task.

**Typical exam question + best answer**
Q: â€œWhy didnâ€™t you use an LLM for intent classification?â€
A: â€œWe chose a ruleâ€‘based classifier for deterministic behavior and low latency. It reduces failure modes (rate limits / prompt drift). We validated coverage by testing example queries across all intent types and iteratively expanding keywords based on observed misclassifications.â€

### 3.3 Entity extraction (implementation detail)
**File:** `src/preprocessing.py`  
**Method:** `extract_entities(query: str) -> Dict[str, List[str]]`

Your extracted fields:
- `players`: list of detected player name strings
- `teams`: currently present but not fully populated by heuristics (your retrieval relies mostly on players/positions/seasons/stats)
- `positions`: normalized to Neo4j position labels (`GK`, `DEF`, `MID`, `FWD`)
- `seasons`: from known seasons list
- `stats`: goals/assists/points/clean sheets/saves/minutes
- `numbers`: digits extracted from query

**Player name extraction logic (key improvement)**
- Split query into words.
- Scan leftâ€‘toâ€‘right.
- When a word starts with a capital letter, collect it and all consecutive capitalized words as a â€œname candidateâ€.
- If candidate length â‰¥ 2, add to players list.
- If no multiâ€‘word players found, fallback: single capitalized words > 3 chars excluding â€œTell/Show/Who/Which/What/Theâ€.

**What to show in slides (fast proof)**
- Example:
  - Input: â€œTell me about Erling Haalandâ€™s performanceâ€
  - Output entities: `players=["Erling Haaland"]`

**Known limitations (say them confidently)**
- If user types everything lowercase (e.g., â€œhaalandâ€), the multiâ€‘word name heuristic may fail.
- Team extraction isnâ€™t as strong as player extraction (team queries depend on how team names are referenced).

**Mitigations you can mention (without claiming you implemented them)**
- Fuzzy matching in Cypher (`CONTAINS`) helps when partial names are extracted.
- Text embedding retrieval still returns semantically relevant players even when entity extraction is weak.

### 3.4 Input embedding (implementation detail)
**Model:** SentenceTransformer `all-MiniLM-L6-v2`  
**Dim:** 384  
**Method:** `embed_query(query)` â†’ list[float]

**Critical stability detail:** `device='cpu'` is used in model initialization to avoid device/meta tensor issues and keep evaluation reproducible.

### 3.5 Error analysis & improvement attempts (mandatory)
You must have 1 slide with â€œissue â†’ evidence â†’ fixâ€. Use these (they are real to your project):

1) **Entity extraction bug**
- Symptom: â€œTell me about Erling Haalandâ€ failed to extract player.
- Root cause: earlier logic missed names at sentence start.
- Fix: consecutive capitalized word scan across the whole sentence.

2) **Least/most handling**
- Symptom: â€œfewest/least/lowestâ€ was misrouted.
- Fix: added `least/fewest/lowest/worst` keywords to `top_players` intent.

---

## 4) Component 2 â€” Graph Retrieval Layer (Baseline + Embeddings)

### 4.1 Retrieval responsibilities (what you must defend in Q&A)
Your retrieval layer must:
1) interpret intent/entities from preprocessing,
2) execute the correct Cypher template (baseline),
3) execute embedding similarity search (semantic),
4) combine both into one output for the UI + LLM.

### 4.2 Baseline retrieval (Cypher)
**File:** `src/retrieval.py`  
**Method:** `baseline_retrieval(user_query) -> (intent, results)`

**Execution sequence (say it like this)**
1) `preprocess()` returns `(intent, entities, embedding)`.
2) `get_query_for_intent(intent, entities)` selects a Cypher template.
3) `_run_cypher_query(cypher_query)` executes Neo4j query.
4) Return intent + list of rows.

**Evidence to show in demo**
- Print/expand â€œintentâ€ and â€œcypher_results countâ€.
- Open â€œShow Source Dataâ€ to show actual rows.

### 4.3 Cypher query templates (â‰¥10)
**File:** `src/cypher_queries.py`

You must show a table in slides: â€œquestion type â†’ query function â†’ what it returnsâ€.

Minimum set you can safely present (10):
1) `get_top_scorers()` â†’ `player, total_goals`
2) `get_top_assisters()` â†’ `player, total_assists`
3) `get_top_points()` â†’ `player, total_points`
4) `get_player_stats(name)` â†’ aggregated matches/goals/assists/points/minutes/cards
5) `get_team_players(team)` â†’ list of players
6) `get_top_clean_sheets(position)` â†’ clean sheets leaders
7) `get_player_fixtures(name)` â†’ gameweek + opponent + performance
8) `compare_players(p1,p2)` â†’ side-by-side aggregated stats
9) `get_position_distribution(team)` â†’ counts by position
10) `get_most_valuable_players(position)` â†’ points efficiency

**Robustness improvements you must highlight**
- Player matching is fuzzy:
  - `get_player_stats()` uses `p.player_name CONTAINS '<name>'` + `LIMIT 1`
  - `compare_players()` uses `CONTAINS` and `LIMIT 2`

**Common failure cases (and what to do live)**
- If a query returns 0 rows:
  1) re-ask using a clearer name (â€œMohamed Salahâ€ not â€œSalahâ€), OR
  2) use embedding toggle to show semantic results still work, OR
  3) show the â€œSearch by nameâ€ query (if you demo it).

### 4.4 Embedding-based retrieval (your selected approach + 2 models)

#### 4.4.1 Chosen approach (state it exactly)
You chose **feature vector embeddings** (not node embeddings).

This choice is defensible because:
- Player performance is naturally represented as a numerical feature vector.
- Similarity queries (cosine) become meaningful and explainable.

#### 4.4.2 Embedding Model A â€” Numerical feature vectors
**File:** `src/create_embeddings.py`  
**Method:** `create_numerical_embeddings(player_stats)`

Features used (13):
- total_points, total_goals, total_assists, total_minutes
- clean_sheets, saves, bonus, bps
- matches_played, influence, creativity, threat, ict_index

Normalization (explain with one line):
For each feature dimension, you apply minâ€“max scaling:

$$x' = \frac{x - \min(x)}{\max(x) - \min(x)}$$

Why normalization matters:
- prevents â€œminutesâ€ from dominating similarity purely due to scale.

#### 4.4.3 Embedding Model B â€” Text embeddings
**File:** `src/create_embeddings.py`  
**Method:** `create_text_embeddings(player_stats)`

You generate a natural language description per player (matches, points, goals, assists, etc.), embed it using SentenceTransformer, and store `text_embedding`.

What to say:
- Numerical embeddings capture *performance profiles*.
- Text embeddings capture *semantic query matching* (â€œgood midfielder for pointsâ€).

#### 4.4.4 Where embeddings are stored (critical to mention)
`store_embeddings_in_neo4j()` sets node properties:
- `p.numerical_embedding` (vector)
- `p.text_embedding` (vector)
and creates vector indexes:
- `player_numerical_index`
- `player_text_index`

#### 4.4.5 How embedding retrieval runs at query time
**File:** `src/retrieval.py`
- `embedding_retrieval_text(user_query)`:
  - embeds the query
  - cosine similarity between query embedding and each playerâ€™s `text_embedding`
- `embedding_retrieval_numerical(user_query)`:
  - extracts a player name
  - compares that playerâ€™s numerical embedding to others

### 4.5 Hybrid retrieval (baseline + embeddings combined)
**File:** `src/retrieval.py`  
**Method:** `hybrid_retrieval(user_query, use_text_embeddings=True)`

This is your â€œintegration proofâ€. In demo you show:
- baseline results (structured facts)
- embedding results (similar players)
- both are visible in UI.

---

## 5) Component 3 â€” LLM Layer (Prompting + 3-model comparison)

### 5.1 What your LLM layer is responsible for
1) build a **grounded prompt** using retrieved context,
2) run multiple models,
3) compare outputs quantitatively + qualitatively.

### 5.2 Structured context construction
**File:** `src/retrieval.py`  
**Method:** `format_context_for_llm(retrieval_results)`

It includes:
- query
- intent
- top 10 Cypher rows (formatted)
- similar players with similarity scores

This is exactly what you show in the UI under â€œShow Retrieved Contextâ€.

### 5.3 Prompt structure (persona + context + task)
**File:** `src/llm_layer.py`  
**Method:** `create_prompt(context, question)`

Your prompt is evaluation-compliant:
- **Persona:** FPL expert assistant
- **Context:** KG results + embeddings
- **Task/Instructions:** use only context, cite stats, say when info is missing

### 5.4 3-model comparison (what to present)
**File:** `src/llm_layer.py`  
**Models:**
- FLANâ€‘T5 Large
- Falcon 7B Instruct
- Phiâ€‘2

#### Quantitative (required)
Use the metrics you already produce:
- `time_seconds` (latency)
- `success` / `error` (availability)

#### Qualitative (required)
Use a scoring rubric (1â€“5). Put this table in slides:

| Model | Faithfulness | Specificity | Readability | Conciseness | Notes |
|---|---:|---:|---:|---:|---|
| FLANâ€‘T5 |  |  |  |  |  |
| Falcon |  |  |  |  |  |
| Phiâ€‘2 |  |  |  |  |  |

**How to fill it live (fast)**
- Faithfulness: does it invent stats not present in context?
- Specificity: does it cite numbers from cypher results?
- Readability: clean structure, bullet points.
- Conciseness: avoids long irrelevant paragraphs.

#### Known LLM limitations (say them early)
- HF inference can rate-limit or fail without a token.
- Mitigation: token input + ability to re-run + focus on integration.

---

## 6) Component 4 â€” UI + Integration (Streamlit)

### 6.1 Required UI items (map to your UI)
Guideline requirement â†’ where it appears:
- â€œUser can write questionâ€ â†’ Chat mode text box
- â€œView KG contextâ€ â†’ Show Retrieved Context toggle
- â€œView final answerâ€ â†’ answer output panel
- â€œSelect a questionâ€ â†’ Example Questions expander (and recent query history)
- â€œIntegration with backendâ€ â†’ retriever/LLM called from UI
- â€œFunctional after answerâ€ â†’ ask multiple queries, history persists

### 6.2 Modes (what each proves)
1) **Chat** proves full pipeline integration.
2) **Compare Embeddings** proves you have two embedding approaches and can show differences.
3) **Graph Stats** proves live Neo4j connectivity and graph content.

### 6.3 Session state + logging (what to say)
- Uses session state to keep retriever and LLM objects initialized.
- Logs queries to `query_log.txt` (evidence of usage + evaluation trace).

### 6.4 Demo plan (the safe sequence)
Always demo in this order:
1) Chat: player stats query
2) Chat: top scorers query
3) Toggle embeddings (text vs numerical) and show â€œsimilar playersâ€ changes
4) Compare Embeddings mode
5) Graph Stats mode

---

## 7) Error Analysis & Improvements (Mandatory Slide)
Use â€œIssue â†’ Root Cause â†’ Fix â†’ Outcomeâ€. Keep it short but specific.

Recommended 4 items:
1) Name extraction failure â†’ improved algorithm â†’ now extracts full names.
2) Exact match no results â†’ `CONTAINS` fuzzy matching â†’ higher hit rate.
3) Missing dependencies (plotly) â†’ requirements install â†’ app runs.
4) WinError 10013 port blocked â†’ run on port 9090 â†’ stable live demo.

---

## 8) Presentation Script (18â€“22 minutes, no overruns)
Use this timeline exactly.

0:00â€“2:00 Architecture
2:00â€“4:00 Preprocessing
4:00â€“7:00 Baseline retrieval (10+ Cypher templates)
7:00â€“10:00 Embeddings (two models + why)
10:00â€“14:00 LLM layer (prompt + 3 models + comparison)
14:00â€“16:00 Error analysis
16:00â€“21:00 Live demo
21:00â€“22:00 Buffer + transition to Q&A

---

## 9) Individual Q&A (Model Answers + What to Open)

### Component 1 (Preprocessing)
Open: `src/preprocessing.py`
- Explain intent scoring and why itâ€™s stable.
- Explain entity extraction with one example and one limitation.
- Explain why CPU embeddings.

### Component 2 (Retrieval)
Open: `src/retrieval.py`, `src/cypher_queries.py`, `src/create_embeddings.py`
- Walk through baseline flow.
- Show two Cypher templates and what they return.
- Explain numerical vs text embeddings, min-max scaling, cosine similarity.

### Component 3 (LLM)
Open: `src/llm_layer.py`
- Show prompt and justify â€œuse only contextâ€.
- Explain the 3-model comparison method and metrics.

### Component 4 (UI)
Open: `app.py`
- Explain modes and why each exists.
- Show session state and query logging.

---

## 10) Commands (Copy/Paste)

Activate venv:
- `.venv\Scripts\Activate.ps1`

Install deps:
- `python -m pip install -r requirements.txt`

Run app (safe port):
- `streamlit run app.py --server.port 9090`

---

## 11) PDF Export

### Option A (always works)
Open `M3_Evaluation_Master_Guide.html` â†’ Ctrl+P â†’ Save as PDF.

### Option B (Opera GX/Edge/Chrome headless)
Run:
- `PowerShell -NoProfile -ExecutionPolicy Bypass -File .\tools\export_master_guide_pdf.ps1`

---

**End of ultraâ€‘detailed guide.**
- Conciseness

**Important limitation to state**
- HuggingFace serverless calls may rateâ€‘limit or fail without a token. Your UI supports a demo mode and token input.

---

## 6) Component 4 â€” UI + Integration (Streamlit)

### 6.1 What the UI must show (per guideline)
- Use case is reflected (FPL Q&A).
- User can write a question.
- UI shows KGâ€‘retrieved context.
- UI shows final LLM answer.
- UI remains functional after answering.
- UI integrates the pipeline backend.

### 6.2 Your UI structure
File: `app.py`
- Modes:
  1) **ğŸ’¬ Chat**: full pipeline
  2) **ğŸ”¬ Compare Embeddings**: sideâ€‘byâ€‘side similarity results
  3) **ğŸ“Š Graph Stats**: counts + plots from Neo4j
- Sidebar controls:
  - Toggle text vs numerical embeddings
  - Show context
  - Show source data
  - Model selector (includes mock)
  - Token input
- Query logging: appends to `query_log.txt`

### 6.3 â€œStill functional after answerâ€ proof
- Ask 3â€“4 different questions sequentially.
- Show sidebar â€œRecent Queriesâ€ updates.

---

## 7) Error Analysis & Improvements (Mandatory Slide)
You must present errors + fixes as *engineering* proof, not as drama.

### 7.1 Mustâ€‘mention issues you already hit
- **No results due to exact name matching** â†’ switched to `CONTAINS`.
- **Entity extraction missed names** â†’ consecutiveâ€‘capitalized extraction.
- **Runtime dependency missing** (e.g., plotly) â†’ install via requirements.
- **Port binding blocked (WinError 10013)** â†’ run Streamlit on a higher port.

### 7.2 â€œFaultâ€‘proofâ€ operating checklist (prevents live demo disasters)
Before evaluation:
- Confirm dependencies installed: `pip install -r requirements.txt`
- Confirm Streamlit launches on a safe port:
  - If port 8501 blocked: `streamlit run app.py --server.port 9090`
- Confirm Neo4j credentials in `config.txt`
- Confirm internet access (for HF models); keep mock fallback ready
- Run one test question endâ€‘toâ€‘end

---

## 8) Presentation Script (18â€“22 Minutes) â€” Exact Timing
Use this exactly; do not improvise on timing.

### 0:00â€“2:00 â€” Highâ€‘Level Architecture
- Show pipeline diagram.
- State: hybrid retrieval (Cypher + embeddings), structured prompt, UI integration.

### 2:00â€“4:00 â€” Input Preprocessing
- Intent classifier: keyword scoring.
- Entity extraction: consecutive capitalized names + FPL positions.
- Embedding: SentenceTransformer.
- Mention 1 improvement.

### 4:00â€“7:00 â€” Baseline Retrieval (Cypher)
- Show table of 10+ query templates.
- Show 1â€“2 query snippets.
- Mention fuzzy matching improvement.

### 7:00â€“10:00 â€” Embedding Retrieval
- State: feature vector embeddings.
- Compare numerical vs text embeddings.
- Show one similarity result example.

### 10:00â€“14:00 â€” LLM Layer
- Context construction.
- Prompt structure (persona/context/task).
- 3â€‘model comparison: speed + qualitative rubric.

### 14:00â€“16:00 â€” Error Analysis & Improvements
- 4 bullets: issue â†’ fix.

### 16:00â€“21:00 â€” Live Demo (must be live)
**Demo order (repeatable):**
1) Start Chat mode.
2) Ask: â€œTell me about Erling Haalandâ€™s performanceâ€ â†’ show intent/entities, cypher results, similar players, answer.
3) Ask: â€œWho scored the most goals in 2022â€‘23?â€ â†’ show top list.
4) Switch embeddings toggle and reâ€‘ask a query â†’ show changed similar players.
5) Compare Embeddings mode: type a known star, show charts.
6) Graph Stats mode: show counts + one chart.

Stop by 21â€“22 minutes.

---

## 9) Individual Q&A â€” What Each Member Must Know

### Component 1 (Preprocessing) â€” likely questions
- Walk through `classify_intent()`; why ruleâ€‘based.
- Show entity extraction; why it handles names at sentence start.
- What happens with typos / lowercase names.
- Why this embedding model; why CPU.

### Component 2 (Retrieval) â€” likely questions
- How do you choose the Cypher query from intent/entities.
- Show at least 2 templates.
- How do you combine baseline + embeddings.
- Why feature vectors (not node embeddings).
- Explain normalization and cosine similarity.

### Component 3 (LLM) â€” likely questions
- Explain the prompt structure.
- How do you ensure faithfulness (answer only from context).
- Show how you measure time and compare models.
- What are failure modes (rate limit, model errors) and your fallback.

### Component 4 (UI) â€” likely questions
- How the UI calls retriever and LLM.
- Why session state is used.
- How you keep UI functional after an answer.
- What happens if Neo4j fails / internet fails.

---

## 10) â€œNo Room for Errorâ€ â€” Final Day Checklist

### 10.1 Technical checklist (do this before you leave home)
- Start venv.
- Run Streamlit on a knownâ€‘working port:
  - `streamlit run app.py --server.port 9090`
- Open `http://localhost:9090`.
- Run 2 questions endâ€‘toâ€‘end.
- Confirm Neo4j connection works.
- Keep `config.txt` correct.

### 10.2 Evaluation behavior checklist
- Everyone knows their component boundaries.
- Only the asked person answers.
- If something breaks: state the issue, apply the prepared fix, continue.

---

## 11) Commands (Copy/Paste)

### Activate venv (PowerShell)
- `.venv\Scripts\Activate.ps1`

### Install dependencies
- `python -m pip install -r requirements.txt`

### Run app (safe port)
- `streamlit run app.py --server.port 9090`

---

## 12) Export to PDF (Reliable Method)
You have two options:

### Option A (always works): Browser print
1) Open the HTML file (we will generate it): `M3_Evaluation_Master_Guide.html`
2) Press **Ctrl+P** â†’ choose **Save as PDF**

### Option B (one command): Edge headless export
Run the script we will generate: `tools\export_master_guide_pdf.ps1`

---

**End of document.**
